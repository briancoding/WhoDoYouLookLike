{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "얼굴감정인식.ipynb",
      "provenance": [],
      "mount_file_id": "1JuW7nMngxPmgvjYL6UkXjvbS9Ml5Rp9O",
      "authorship_tag": "ABX9TyN+69w5vu0BnoeKZ7r8Re9x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/briancoding/WhoDoYouLookLike/blob/main/%EC%96%BC%EA%B5%B4%EA%B0%90%EC%A0%95%EC%9D%B8%EC%8B%9D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fW8FVek_TsCy"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.optimizers import Adam  \n",
        "from keras.preprocessing.image import ImageDataGenerator"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-BebewuZdAM"
      },
      "source": [
        "# initialize image data generator with rescaling"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2HIwG-kZy1r"
      },
      "source": [
        "train_data_gen = ImageDataGenerator(rescale=1./255)\n",
        "validation_data_gen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilIe36rRZ58f"
      },
      "source": [
        "# preprocess all test images"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk64VI8SZ8AS",
        "outputId": "78a2c48f-84aa-42f5-b0cb-17eb5ce95efc"
      },
      "source": [
        "train_generator = train_data_gen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/EmotionData/train',\n",
        "    target_size=(48,48),\n",
        "    batch_size=64,\n",
        "    color_mode='grayscale',\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 9853 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOUFJpLiaahb"
      },
      "source": [
        "#preprocess all train images"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hOmjsfWacP5",
        "outputId": "d626bbc9-cfd8-4af5-f6bc-561cea073df2"
      },
      "source": [
        "validation_generator=validation_data_gen.flow_from_directory(\n",
        "    '/content/drive/MyDrive/Colab Notebooks/EmotionData/test',\n",
        "    target_size=(48,48),\n",
        "    batch_size=64,\n",
        "    color_mode='grayscale',\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7178 images belonging to 7 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUB3cJbcapk_",
        "outputId": "eb6fac92-292e-47dc-e06f-cf171d6e21e8"
      },
      "source": [
        "#create model structure\n",
        "emotion_model = Sequential()\n",
        "\n",
        "emotion_model.add(Conv2D(32,kernel_size=(3,3),activation='relu',input_shape=(48,48,1)))\n",
        "emotion_model.add(Conv2D(64,kernel_size=(3,3),activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "emotion_model.add(Conv2D(128,kernel_size=(3,3),activation='relu'))\n",
        "emotion_model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "emotion_model.add(Dropout(0.25))\n",
        "\n",
        "emotion_model.add(Flatten())\n",
        "emotion_model.add(Dense(1024,activation='relu'))\n",
        "emotion_model.add(Dropout(0.5))\n",
        "emotion_model.add(Dense(7,activation='softmax'))\n",
        "\n",
        "emotion_model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001,decay=1e-6),metrics=['accuracy'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2ZXdkCKhNcV"
      },
      "source": [
        "#Train the neural network/model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v2fD5fIjhQtN",
        "outputId": "ea77ea43-f5da-4fc8-e4f8-89103c8ebd9b"
      },
      "source": [
        "emotion_model_info = emotion_model.fit_generator(\n",
        "    train_generator,\n",
        "    steps_per_epoch=9296 // 64,\n",
        "    epochs=10,\n",
        "    validation_data=validation_generator,\n",
        "    validation_steps=7178//64\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "145/145 [==============================] - 2628s 18s/step - loss: 1.7777 - accuracy: 0.3412 - val_loss: 1.9026 - val_accuracy: 0.1334\n",
            "Epoch 2/10\n",
            "145/145 [==============================] - 102s 703ms/step - loss: 1.7493 - accuracy: 0.3470 - val_loss: 1.9069 - val_accuracy: 0.1336\n",
            "Epoch 3/10\n",
            "145/145 [==============================] - 35s 239ms/step - loss: 1.6992 - accuracy: 0.3550 - val_loss: 1.8424 - val_accuracy: 0.2263\n",
            "Epoch 4/10\n",
            "145/145 [==============================] - 32s 222ms/step - loss: 1.6380 - accuracy: 0.3904 - val_loss: 1.7894 - val_accuracy: 0.2676\n",
            "Epoch 5/10\n",
            "145/145 [==============================] - 32s 222ms/step - loss: 1.5791 - accuracy: 0.4273 - val_loss: 1.7561 - val_accuracy: 0.2782\n",
            "Epoch 6/10\n",
            "145/145 [==============================] - 32s 223ms/step - loss: 1.5218 - accuracy: 0.4464 - val_loss: 1.6442 - val_accuracy: 0.3443\n",
            "Epoch 7/10\n",
            "145/145 [==============================] - 32s 222ms/step - loss: 1.4909 - accuracy: 0.4595 - val_loss: 1.6555 - val_accuracy: 0.3242\n",
            "Epoch 8/10\n",
            "145/145 [==============================] - 32s 222ms/step - loss: 1.4523 - accuracy: 0.4799 - val_loss: 1.6179 - val_accuracy: 0.3563\n",
            "Epoch 9/10\n",
            "145/145 [==============================] - 33s 226ms/step - loss: 1.4284 - accuracy: 0.4817 - val_loss: 1.5703 - val_accuracy: 0.3749\n",
            "Epoch 10/10\n",
            "145/145 [==============================] - 33s 229ms/step - loss: 1.3898 - accuracy: 0.4960 - val_loss: 1.5753 - val_accuracy: 0.3838\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JNc5s_cThqNB"
      },
      "source": [
        "#save model structure in jason file"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLsJQvobhyse"
      },
      "source": [
        "model_json = emotion_model.to_json()\n",
        "with open('emotion_model.json','w') as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "emotion_model.save_weights('emotion_model.h5')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OS1XfRJqiRJ1",
        "outputId": "58d4e61c-2ff7-45f1-8a85-7df25e0ef355"
      },
      "source": [
        "# Test Emotion Detector\n",
        "! git clone https://github.com/datamagic2020/Emotion_detection_with_CNN.git"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Emotion_detection_with_CNN'...\n",
            "remote: Enumerating objects: 25, done.\u001b[K\n",
            "remote: Counting objects: 100% (25/25), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 25 (delta 6), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (25/25), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUqBdmV0iUUL"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from keras.models import model_from_json"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw6st88OiaFX"
      },
      "source": [
        "emotion_dict={0:'Angry',1:'Disgusted',2:'Fearful',3:'Happy',4:'Neutral',5:'Sad',6:'Surprised'}"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5d8CpreNinqo"
      },
      "source": [
        "#load json and create model"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDfSSqFRlTby"
      },
      "source": [
        "json_file = open('/content/emotion_model.json','r')\n",
        "loaded_model_json= json_file.read()\n",
        "json_file.close()\n",
        "emotion_model = model_from_json(loaded_model_json)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ5lokqglgT7"
      },
      "source": [
        "#load weights into new model"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--7a25jjliXf",
        "outputId": "7551e5a8-5456-4e67-b80b-e5662f27961d"
      },
      "source": [
        "emotion_model.load_weights('/content/emotion_model.h5')\n",
        "print('Loaded modedl from disk')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded modedl from disk\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQjUsLoQlpCy"
      },
      "source": [
        "#Start the webcam feed\n",
        "cap = cv2.VideoCapture(0)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8TY6FHn4Hv4"
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "18dBZdrj5A1N"
      },
      "source": [
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxPdNGFH5ESg",
        "outputId": "dcc53aed-97d9-425e-8953-705b03137385"
      },
      "source": [
        "!curl -o logo.png https://colab.research.google.com/img/colab_favicon_256px.png\n",
        "import cv2"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "\r  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\r100  4534  100  4534    0     0   210k      0 --:--:-- --:--:-- --:--:--  210k\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqByUlZGluHb"
      },
      "source": [
        "cap = cv2.VideoCapture('여기에 비디오 링크 넣기')\n",
        "while True:\n",
        "  ret, frame = cap.read()\n",
        "  frame = cv2.resize(frame, (720,720))\n",
        "  if not ret:\n",
        "    break\n",
        "  face_detector = cv2.CascadeClassifier('/content/Emotion_detection_with_CNN/haarcascades/haarcascade_frontalface_default.xml')\n",
        "  gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "  num_faces = face_detector.detectMultiScale(gray_frame, scaleFactor=1.3, minNeighbors=5)\n",
        "\n",
        "  for (x,y,w,h) in num_faces:\n",
        "    cv2.rectangle(frame, (x,y-50),(x+w,y+h+10),(0,255,0),4)\n",
        "    roi_gray_frame =gray_frame[y:y +h,x:x+w]\n",
        "    cropped_img = np.expand_dims(np.expand_dims(cv2.resize(roi_gray_frame,(48,48)),-1),0)\n",
        "\n",
        "    #prediction\n",
        "    emotion_prediction = emotion_model.predict(cropped_img)\n",
        "    maxindex = int(np.argmax(emotion_prediction))\n",
        "    cv2.putText(frame,emotion_dict[maxindex],(x+5,y-20),cv2.FONT_HERSHEY_SIMPLEX,1,(255,0,0),2,cv2.LINE_AA)\n",
        "\n",
        "  cv2_imshow(frame)\n",
        "  if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWPPjQP-mloZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}